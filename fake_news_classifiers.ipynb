{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 821 ms, sys: 166 ms, total: 988 ms\n",
      "Wall time: 814 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import resources.visualize as vis\n",
    "import resources.preprocessor as prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 8000\n",
    "\n",
    "news = pd.read_csv('data.csv', nrows=sample_size)\n",
    "scores = {}\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.labels_bar_plot(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bag Of Words Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "- remove html code\n",
    "- remove punctuation\n",
    "- make everything lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.dropna() # remove rows with at least one missing element\n",
    "news['text'] = news['tweet'].apply(vis.clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.encode_labels(news)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorization in NLP is the conversion of text input data into vectors of real numbers, the format supported by ML models.\n",
    "I will start by using the bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = prep.bag_of_words_vectorize(news)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "pred = mnb.predict(X_test)\n",
    "scores[\"bow_mnb\"] = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "vis.plot_cf_matrix(y_test, pred)\n",
    "vis.plot_pie_chart(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(mnb, 'models/bow_mnb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "pred = lr.predict(X_test)\n",
    "scores[\"bow_lr\"] = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "vis.plot_cf_matrix(y_test, pred)\n",
    "vis.plot_pie_chart(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lr, 'models/bow_lr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac = PassiveAggressiveClassifier(max_iter=50)\n",
    "pac.fit(X_train, y_train)\n",
    "pred = pac.predict(X_test)\n",
    "scores[\"bow_pac\"] = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "\n",
    "vis.plot_cf_matrix(y_test, pred)\n",
    "vis.plot_pie_chart(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pac, 'models/bow_pac.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(50,50,50), max_iter=500, activation = 'relu', solver='adam', random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "pred = mlp.predict(X_test)\n",
    "scores[\"bow_mlp\"] = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "vis.plot_cf_matrix(y_test, pred)\n",
    "vis.plot_pie_chart(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(mlp, 'models/bow_mlp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'learning_rate': ['constant','adaptive']\n",
    "}\n",
    "\n",
    "gscv = GridSearchCV(mlp, parameter_space, n_jobs=-1)\n",
    "gscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best params: ', gscv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gscv, 'models/tuned_mlp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gscv.predict(X_test)\n",
    "\n",
    "vis.plot_cf_matrix(y_test, pred)\n",
    "vis.plot_pie_chart(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TF-IDF Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing using the Term Frequency Inverse Document Frequency (TFIDF) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prep.tfidf_vectorize(news)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "pred = mnb.predict(X_test)\n",
    "scores[\"tfidf_mnb\"] = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "vis.plot_cf_matrix(y_test, pred)\n",
    "vis.plot_pie_chart(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(mnb, 'models/tfidf_mnb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "pred = lr.predict(X_test)\n",
    "scores[\"tfidf_lr\"] = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "vis.plot_cf_matrix(y_test, pred)\n",
    "vis.plot_pie_chart(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(mnb, 'models/tfidf_lr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac = PassiveAggressiveClassifier(max_iter=50)\n",
    "pac.fit(X_train, y_train)\n",
    "pred = pac.predict(X_test)\n",
    "scores[\"tfidf_pac\"] = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "vis.plot_cf_matrix(y_test, pred)\n",
    "vis.plot_pie_chart(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(mnb, 'models/tfidf_pac.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(50,50,50), max_iter=500, activation = 'relu', solver='adam', random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "pred = mlp.predict(X_test)\n",
    "scores[\"tfidf_mlp\"] = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "vis.plot_cf_matrix(y_test, pred)\n",
    "vis.plot_pie_chart(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(mnb, 'models/tfidf_mlp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. BERT Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_sklearn import BertClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the imports above are not working, uncoment the code below and run it to install bert-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "code taken from https://colab.research.google.com/drive/1-wTNA-qYmOBdSYG7sRhIdOrxcgPpcl6L\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "!git clone -b master https://github.com/charles9n/bert-sklearn\n",
    "!cd bert-sklearn; pip install .\n",
    "import os\n",
    "os.chdir(\"bert-sklearn\")\n",
    "print(os.listdir())\n",
    "\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time taken = 26m 35.3s\n",
    "\n",
    "news = pd.read_csv('data.csv', nrows=8000)\n",
    "\n",
    "news = news.dropna() # remove rows with at least one missing element\n",
    "news['text'] = news['tweet'].apply(vis.clean)\n",
    "\n",
    "prep.encode_labels(news)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(news['tweet'], news['label'], test_size=0.5)\n",
    "\n",
    "bert = BertClassifier(max_seq_length=64, train_batch_size=16)\n",
    "bert = bert.fit(X_train, y_train)\n",
    "accy = bert.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(bert, 'models/bert.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bert.predict(X_test)\n",
    "scores[\"bert\"] = metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_cf_matrix(y_test, y_pred)\n",
    "vis.plot_pie_chart(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1])}\n",
    "\n",
    "print (\"{:<10}      {:<10}\".format(\"model\",\"score\"))\n",
    "for k, v in d.items():\n",
    "    print (\"{:<10}      {:<10}\".format(k,v))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
